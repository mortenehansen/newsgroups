{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis of sci-kit learn's 20 newsgroups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\AzureML\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Choose a few categories or all categories\n",
    "categories=['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "#categories=None\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: rych@festival.ed.ac.uk (R Hawkes)\\nSubject: 3DS: Where did all the texture rules go?\\nLines: 21\\n\\nHi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\\n\\n======================================================================\\nRycharde Hawkes\\t\\t\\t\\temail: rych@festival.ed.ac.uk\\nVirtual Environment Laboratory\\nDept. of Psychology\\t\\t\\tTel  : +44 31 650 3426\\nUniv. of Edinburgh\\t\\t\\tFax  : +44 31 667 0150\\n======================================================================\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training data comes as a list of Strings. This shows an example of a such String.\n",
    "documents = newsgroups_train.data\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of the targets (news groups)\n",
    "target_names = newsgroups_train.target_names\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " ':',\n",
       " 'rych',\n",
       " '@',\n",
       " 'festival.ed.ac.uk',\n",
       " '(',\n",
       " 'r',\n",
       " 'hawkes',\n",
       " ')',\n",
       " 'subject',\n",
       " ':',\n",
       " '3ds',\n",
       " ':',\n",
       " 'where',\n",
       " 'did',\n",
       " 'all',\n",
       " 'the',\n",
       " 'texture',\n",
       " 'rules',\n",
       " 'go']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the documents by setting them to lowercase and tokenize them. \n",
    "documents = [d.lower() for d in documents]\n",
    "documents = [word_tokenize(d) for d in documents]\n",
    "\n",
    "documents[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5480217, 7873910)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the gensim Word2Vec class to train an embedding.\n",
    "\n",
    "vector_dim = 200\n",
    "\n",
    "model = Word2Vec(\n",
    "    documents,\n",
    "    size=vector_dim,\n",
    "    window=3,\n",
    "    min_count=2,\n",
    "    workers=5)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\AzureML\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Grab the word vectors. These will work as the embeddings in the neural network.\n",
    "WordVectors = model[model.wv.vocab]\n",
    "\n",
    "# vocab is a dictionary with the vocabulary defined by the Word2Vec model as keys.\n",
    "vocab = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the documents into lists of integers where each integer is the index of the word in the embedding.\n",
    "# The lists are sequences must all have the same length, MAX_SEQUENCE_LENGTH, so those that are too long\n",
    "# are truncated while those that are too short are padded with zeros.\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "word_index = {t: i for i,t in enumerate(list(vocab))}\n",
    "sequences = [[word_index.get(t,0) for t in document] for document in documents]\n",
    "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "# Prepare the targets by giving them a one hot encoding.\n",
    "y_train = newsgroups_train.target\n",
    "#y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data\n",
    "\n",
    "test_docs = newsgroups_test.data\n",
    "test_label = newsgroups_test.target\n",
    "\n",
    "test_docs = [d.lower() for d in test_docs]\n",
    "test_docs = [word_tokenize(d) for d in test_docs]\n",
    "    \n",
    "test_sequences = [[word_index.get(t,0) for t in document] for document in test_docs]\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "y_test = test_label\n",
    "#y_test = to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         1,   2,   3,   4,   5,   6,   7,   8,   9,   1,  10,   1,  11,\n",
       "        12,  13,  14,  15,  16,  17,  18,  19,   1,  20,  21,  22,  23,\n",
       "        24,  25,  26,  27,  28,  29,  30,  31,  32,   5,  33,  13,  34,\n",
       "        35,  36,  37,  38,   8,  39,  31,  40,  41,  26,  42,  28,  43,\n",
       "        44,  45,   0,  10,  22,  46,  47,  48,  31,  49,  50,  51,  52,\n",
       "        53,  54,  27,  28,  30,  39,  31,  55,  41,  56,   0,  47,  57,\n",
       "        53,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  14,  40,\n",
       "        41,  18,  68,  64,  69,  70,  67,  14,  71,  72,  73,  15,  16,\n",
       "        67,  14,  55,  41,  53,  23,  74,  75,  39,  76,  77,  39,  78,\n",
       "        14,  15,  79,  63,  22,  58,  59,  80,  14,  81,  82,  14,  55,\n",
       "        41,  18,  64,  14,   0,  41,  81,  83,   0,  84,  18,   2,  85,\n",
       "         0,   7,  86,   1,   2,   3,   4,  87,  88,  89,  90,  53,  91,\n",
       "        92,  93,   1,  94,  95,  96,   0,  97,  53,  91,  98,  99,   1,\n",
       "        94,  95, 100,   0,  85])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 1, 1, 2, 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " ':',\n",
       " 'dpw',\n",
       " '@',\n",
       " 'sei.cmu.edu',\n",
       " '(',\n",
       " 'david',\n",
       " 'wood',\n",
       " ')',\n",
       " 'subject',\n",
       " ':',\n",
       " 'request',\n",
       " 'for',\n",
       " 'support',\n",
       " 'organization',\n",
       " ':',\n",
       " 'software',\n",
       " 'engineering',\n",
       " 'institute',\n",
       " 'lines',\n",
       " ':',\n",
       " '35',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'request',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'see',\n",
       " 'charley',\n",
       " 'wingate',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'the',\n",
       " '``',\n",
       " 'charley',\n",
       " 'challenges',\n",
       " \"''\",\n",
       " '(',\n",
       " 'and',\n",
       " 'judging',\n",
       " 'from',\n",
       " 'my',\n",
       " 'e-mail',\n",
       " ',',\n",
       " 'there',\n",
       " 'appear',\n",
       " 'to',\n",
       " 'be',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'few',\n",
       " 'of',\n",
       " 'you',\n",
       " '.',\n",
       " ')',\n",
       " 'it',\n",
       " 'is',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'mr.',\n",
       " 'wingate',\n",
       " 'intends',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'post',\n",
       " 'tangential',\n",
       " 'or',\n",
       " 'unrelated',\n",
       " 'articles',\n",
       " 'while',\n",
       " 'ingoring',\n",
       " 'the',\n",
       " 'challenges',\n",
       " 'themselves',\n",
       " '.',\n",
       " 'between',\n",
       " 'the',\n",
       " 'last',\n",
       " 'two',\n",
       " 're-postings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'challenges',\n",
       " ',',\n",
       " 'i',\n",
       " 'noted',\n",
       " 'perhaps',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'or',\n",
       " 'more',\n",
       " 'posts',\n",
       " 'by',\n",
       " 'mr.',\n",
       " 'wingate',\n",
       " ',',\n",
       " 'none',\n",
       " 'of',\n",
       " 'which',\n",
       " 'answered',\n",
       " 'a',\n",
       " 'single',\n",
       " 'challenge',\n",
       " '.',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'unmistakable',\n",
       " 'to',\n",
       " 'me',\n",
       " 'that',\n",
       " 'mr.',\n",
       " 'wingate',\n",
       " 'hopes',\n",
       " 'that',\n",
       " 'the',\n",
       " 'questions',\n",
       " 'will',\n",
       " 'just',\n",
       " 'go',\n",
       " 'away',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'is',\n",
       " 'doing',\n",
       " 'his',\n",
       " 'level',\n",
       " 'best',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'subject',\n",
       " '.',\n",
       " 'given',\n",
       " 'that',\n",
       " 'this',\n",
       " 'seems',\n",
       " 'a',\n",
       " 'rather',\n",
       " 'common',\n",
       " 'net.theist',\n",
       " 'tactic',\n",
       " ',',\n",
       " 'i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'we',\n",
       " 'impress',\n",
       " 'upon',\n",
       " 'him',\n",
       " 'our',\n",
       " 'desire',\n",
       " 'for',\n",
       " 'answers',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'following',\n",
       " 'manner',\n",
       " ':',\n",
       " '1.',\n",
       " 'ignore',\n",
       " 'any',\n",
       " 'future',\n",
       " 'articles',\n",
       " 'by',\n",
       " 'mr.',\n",
       " 'wingate',\n",
       " 'that',\n",
       " 'do',\n",
       " 'not',\n",
       " 'address',\n",
       " 'the',\n",
       " 'challenges',\n",
       " ',',\n",
       " 'until',\n",
       " 'he',\n",
       " 'answers',\n",
       " 'them',\n",
       " 'or',\n",
       " 'explictly',\n",
       " 'announces',\n",
       " 'that',\n",
       " 'he',\n",
       " 'refuses',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " '.',\n",
       " '--',\n",
       " 'or',\n",
       " '--',\n",
       " '2.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'must',\n",
       " 'respond',\n",
       " 'to',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'articles',\n",
       " ',',\n",
       " 'include',\n",
       " 'within',\n",
       " 'it',\n",
       " 'something',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'the',\n",
       " 'following',\n",
       " ':',\n",
       " '``',\n",
       " 'please',\n",
       " 'answer',\n",
       " 'the',\n",
       " 'questions',\n",
       " 'posed',\n",
       " 'to',\n",
       " 'you',\n",
       " 'in',\n",
       " 'the',\n",
       " 'charley',\n",
       " 'challenges',\n",
       " '.',\n",
       " \"''\",\n",
       " 'really',\n",
       " ',',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'humiliate',\n",
       " 'anyone',\n",
       " 'here',\n",
       " ',',\n",
       " 'i',\n",
       " 'just',\n",
       " 'want',\n",
       " 'some',\n",
       " 'honest',\n",
       " 'answers',\n",
       " '.',\n",
       " 'you',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'think',\n",
       " 'that',\n",
       " 'honesty',\n",
       " 'would',\n",
       " 'be',\n",
       " 'too',\n",
       " 'much',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'from',\n",
       " 'a',\n",
       " 'devout',\n",
       " 'christian',\n",
       " ',',\n",
       " 'would',\n",
       " 'you',\n",
       " '?',\n",
       " 'nevermind',\n",
       " ',',\n",
       " 'that',\n",
       " 'was',\n",
       " 'a',\n",
       " 'rhetorical',\n",
       " 'question',\n",
       " '.',\n",
       " '--',\n",
       " 'dave',\n",
       " 'wood']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM, Dropout\n",
    "\n",
    "k_model = Sequential()\n",
    "\n",
    "k_model.add(Embedding(len(vocab), vector_dim, weights=[WordVectors], \n",
    "                      input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "\n",
    "k_model.add(LSTM(vector_dim, return_sequences=True))\n",
    "#k_model.add(Dropout(0.2))\n",
    "\n",
    "k_model.add(LSTM(vector_dim, return_sequences=True))\n",
    "#k_model.add(Dropout(0.2))\n",
    "\n",
    "k_model.add(Flatten())\n",
    "\n",
    "k_model.add(Dense(512, activation=tf.nn.relu))\n",
    "\n",
    "#k_model.add(Dense(512, activation=tf.nn.relu))\n",
    "k_model.add(Dense(len(target_names), activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2034 samples, validate on 1353 samples\n",
      "Epoch 1/10\n",
      "2034/2034 [==============================] - 62s 31ms/step - loss: 1.3841 - acc: 0.2852 - val_loss: 1.3817 - val_acc: 0.2912\n",
      "Epoch 2/10\n",
      "2034/2034 [==============================] - 59s 29ms/step - loss: 1.3799 - acc: 0.2915 - val_loss: 1.3782 - val_acc: 0.2912\n",
      "Epoch 3/10\n",
      "2034/2034 [==============================] - 58s 29ms/step - loss: 1.3771 - acc: 0.2915 - val_loss: 1.3758 - val_acc: 0.2912\n",
      "Epoch 4/10\n",
      "2034/2034 [==============================] - 59s 29ms/step - loss: 1.3752 - acc: 0.2915 - val_loss: 1.3741 - val_acc: 0.2912\n",
      "Epoch 5/10\n",
      "2034/2034 [==============================] - 59s 29ms/step - loss: 1.3738 - acc: 0.2915 - val_loss: 1.3732 - val_acc: 0.2912\n",
      "Epoch 6/10\n",
      "2034/2034 [==============================] - 59s 29ms/step - loss: 1.3729 - acc: 0.2915 - val_loss: 1.3724 - val_acc: 0.2912\n",
      "Epoch 7/10\n",
      "2034/2034 [==============================] - 58s 29ms/step - loss: 1.3723 - acc: 0.2915 - val_loss: 1.3719 - val_acc: 0.2912\n",
      "Epoch 8/10\n",
      "2034/2034 [==============================] - 58s 29ms/step - loss: 1.3719 - acc: 0.2915 - val_loss: 1.3716 - val_acc: 0.2912\n",
      "Epoch 9/10\n",
      "2034/2034 [==============================] - 59s 29ms/step - loss: 1.3715 - acc: 0.2915 - val_loss: 1.3714 - val_acc: 0.2912\n",
      "Epoch 10/10\n",
      "2034/2034 [==============================] - 58s 29ms/step - loss: 1.3714 - acc: 0.2915 - val_loss: 1.3712 - val_acc: 0.2912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a6fabe6c88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "k_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose = 1, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AzureML]",
   "language": "python",
   "name": "conda-env-AzureML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
